import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
import os
import pandas as pd
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt

class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }


class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, 
                           hidden_dim, 
                           num_layers=n_layers, 
                           bidirectional=True, 
                           dropout=dropout,
                           batch_first=True)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input_ids, attention_mask):
        embedded = self.dropout(self.embedding(input_ids))
        output, (hidden, cell) = self.lstm(embedded)
        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))
        return self.fc(hidden)


def load_imdb_data():
    # Update the path to include the correct directory structure
    base_path = os.path.join(os.getcwd(), 'Advanced Predictive Analytics', 'Project Files-20250403', 'aclImdb')
    print(f"Looking for dataset in: {base_path}")
    
    if not os.path.exists(base_path):
        print("Dataset directory not found. Please check the path.")
        print("Current directory structure:")
        for root, dirs, files in os.walk(os.path.join(os.getcwd(), 'Advanced Predictive Analytics')):
            print(f"Directory: {root}")
            print(f"Subdirectories: {dirs}")
            print(f"Files: {files}")
            print("---")
        return [], []
    
    texts = []
    labels = []
    
    # Load positive reviews
    pos_path = os.path.join(base_path, 'train', 'pos')
    print(f"Loading positive reviews from: {pos_path}")
    
    if not os.path.exists(pos_path):
        print(f"Positive reviews directory not found at: {pos_path}")
        return [], []
    
    for filename in os.listdir(pos_path)[:1000]:  # Using first 1000 for testing
        if filename.endswith('.txt'):
            with open(os.path.join(pos_path, filename), 'r', encoding='utf-8') as f:
                texts.append(f.read())
                labels.append(1)
    
    # Load negative reviews
    neg_path = os.path.join(base_path, 'train', 'neg')
    print(f"Loading negative reviews from: {neg_path}")
    
    if not os.path.exists(neg_path):
        print(f"Negative reviews directory not found at: {neg_path}")
        return [], []
    
    for filename in os.listdir(neg_path)[:1000]:  # Using first 1000 for testing
        if filename.endswith('.txt'):
            with open(os.path.join(neg_path, filename), 'r', encoding='utf-8') as f:
                texts.append(f.read())
                labels.append(0)
    
    print(f"Loaded {len(texts)} reviews in total")
    return texts, labels


def train_model(model, train_loader, device, num_epochs=5):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    
    # Store metrics for plotting
    train_losses = []
    train_accuracies = []
    
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        correct = 0
        total = 0
        
        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        accuracy = 100 * correct / total
        avg_loss = total_loss / len(train_loader)
        
        train_losses.append(avg_loss)
        train_accuracies.append(accuracy)
        
        print(f'Epoch {epoch+1}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.2f}%')
    
    return train_losses, train_accuracies


# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Initialize tokenizer
print("Initializing tokenizer...")
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Load data
print("Loading data...")
texts, labels = load_imdb_data()

# Create dataset and dataloader
print("Creating dataset and dataloader...")
dataset = TextDataset(texts, labels, tokenizer)
train_loader = DataLoader(dataset, batch_size=32, shuffle=True)

# Initialize model
print("Initializing model...")
model = LSTMModel(
    vocab_size=tokenizer.vocab_size,
    embedding_dim=300,
    hidden_dim=256,
    output_dim=2,  # Binary classification
    n_layers=2,
    dropout=0.3
).to(device)

# Train model
print("Starting training...")
train_losses, train_accuracies = train_model(model, train_loader, device)


# Plot training loss and accuracy
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Training Loss')
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_accuracies, label='Training Accuracy')
plt.title('Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.legend()

plt.tight_layout()
plt.show()

