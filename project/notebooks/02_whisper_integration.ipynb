{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18251870",
   "metadata": {},
   "source": [
    "# Part 2: Speech-to-Text with Whisper\n",
    "\n",
    "This notebook implements the speech-to-text component with Whisper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac8abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import whisper\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join(os.pardir, \"src\"))\n",
    "\n",
    "# Import our custom module\n",
    "from speech_analysis.whisper_processor import WhisperTranscriber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f54a0",
   "metadata": {},
   "source": [
    "## 1. Download Sample Audio Files\n",
    "\n",
    "We will download sample emotional speech files from the RAVDESS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b5fbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file(url, target_path):\n",
    "    \"\"\"Download a file from a URL to a target path.\"\"\"\n",
    "    if os.path.exists(target_path):\n",
    "        print(f\"File already exists at {target_path}\")\n",
    "        return\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "    \n",
    "    with open(target_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    \n",
    "    print(f\"Downloaded {url} to {target_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAVDESS dataset sample files\n",
    "# Emotions: 01=neutral, 03=happy, 04=sad, 05=angry\n",
    "\n",
    "base_url = \"https://zenodo.org/records/1188976/files/\"\n",
    "audio_files = [\n",
    "    # Neutral sample\n",
    "    \"03-01-01-01-01-01-01.wav\",  # Neutral, female\n",
    "    \n",
    "    # Happy sample\n",
    "    \"03-01-03-01-01-01-11.wav\",  # Happy, male\n",
    "    \n",
    "    # Sad sample\n",
    "    \"03-01-04-01-01-01-01.wav\",  # Sad, female\n",
    "    \n",
    "    # Angry sample\n",
    "    \"03-01-05-01-01-01-11.wav\"   # Angry, male\n",
    "]\n",
    "\n",
    "audio_dir = \"../data/audio/samples\"\n",
    "audio_paths = {}\n",
    "\n",
    "# Download the audio files\n",
    "for file_name in audio_files:\n",
    "    url = base_url + file_name\n",
    "    target_path = os.path.join(audio_dir, file_name)\n",
    "    download_file(url, target_path)\n",
    "    \n",
    "    # Map file name to emotion\n",
    "    emotion_code = file_name.split(\"-\")[2]\n",
    "    emotion_map = {\n",
    "        \"01\": \"neutral\",\n",
    "        \"03\": \"happy\",\n",
    "        \"04\": \"sad\",\n",
    "        \"05\": \"angry\"\n",
    "    }\n",
    "    emotion = emotion_map.get(emotion_code, \"unknown\")\n",
    "    audio_paths[target_path] = emotion\n",
    "\n",
    "print(f\"Downloaded {len(audio_files)} audio files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98ddd6",
   "metadata": {},
   "source": [
    "## 2. Initialize Whisper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbffd127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Whisper transcriber\n",
    "transcriber = WhisperTranscriber(model_name=\"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628ed63",
   "metadata": {},
   "source": [
    "## 3. Transcribe Audio Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f76ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "transcriptions = []\n",
    "\n",
    "for audio_path, emotion in audio_paths.items():\n",
    "    # Display audio player\n",
    "    display(Audio(audio_path))\n",
    "    \n",
    "    # Transcribe the audio\n",
    "    result = transcriber.transcribe_file(audio_path)\n",
    "    \n",
    "    # Get filename\n",
    "    filename = os.path.basename(audio_path)\n",
    "    \n",
    "    # Add to our list\n",
    "    transcriptions.append({\n",
    "        \"filename\": filename,\n",
    "        \"emotion\": emotion,\n",
    "        \"text\": result[\"text\"],\n",
    "        \"audio_path\": audio_path\n",
    "    })\n",
    "    \n",
    "    # Print the transcription\n",
    "    print(f\"File: {filename} (Emotion: {emotion})\")\n",
    "    print(f\"Transcription: {result[\"text\"]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Create DataFrame\n",
    "transcriptions_df = pd.DataFrame(transcriptions)\n",
    "transcriptions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4da0175",
   "metadata": {},
   "source": [
    "## 4. Simple Text Sentiment Analysis\n",
    "\n",
    "As a placeholder, we will use a simple rule-based sentiment analyzer. In a complete implementation, you would integrate with your model from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sentiment analysis function\n",
    "def simple_sentiment_analysis(text):\n",
    "    \"\"\"A simple rule-based sentiment analyzer for demonstration.\"\"\"\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Define positive and negative word lists\n",
    "    positive_words = [\"happy\", \"good\", \"great\", \"joy\", \"love\", \"excellent\", \"positive\"]\n",
    "    negative_words = [\"sad\", \"bad\", \"terrible\", \"awful\", \"hate\", \"dislike\", \"negative\"]\n",
    "    \n",
    "    # Count positive and negative words\n",
    "    positive_count = sum(1 for word in positive_words if word in text)\n",
    "    negative_count = sum(1 for word in negative_words if word in text)\n",
    "    \n",
    "    # Calculate sentiment score (0 to 1)\n",
    "    if positive_count + negative_count == 0:\n",
    "        return 0.5  # Neutral if no positive or negative words\n",
    "    else:\n",
    "        return positive_count / (positive_count + negative_count)\n",
    "\n",
    "# Apply simple sentiment analysis to transcriptions\n",
    "transcriptions_df[\"sentiment_score\"] = transcriptions_df[\"text\"].apply(simple_sentiment_analysis)\n",
    "\n",
    "# Convert score to label (positive, neutral, negative)\n",
    "def score_to_label(score):\n",
    "    if score >= 0.7:\n",
    "        return \"positive\"\n",
    "    elif score <= 0.3:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "        \n",
    "transcriptions_df[\"sentiment\"] = transcriptions_df[\"sentiment_score\"].apply(score_to_label)\n",
    "\n",
    "# Display results\n",
    "transcriptions_df[[\"filename\", \"emotion\", \"text\", \"sentiment_score\", \"sentiment\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f31b9f9",
   "metadata": {},
   "source": [
    "## 5. Create an End-to-End Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_speech_sentiment(audio_path):\n",
    "    \"\"\"End-to-end pipeline for speech sentiment analysis.\"\"\"\n",
    "    # Display audio\n",
    "    display(Audio(audio_path))\n",
    "    \n",
    "    # Step 1: Transcribe audio\n",
    "    print(\"Transcribing audio...\")\n",
    "    result = transcriber.transcribe_file(audio_path)\n",
    "    transcription = result[\"text\"]\n",
    "    print(f\"Transcription: {transcription}\")\n",
    "    \n",
    "    # Step 2: Analyze sentiment\n",
    "    print(\"Analyzing sentiment...\")\n",
    "    sentiment_score = simple_sentiment_analysis(transcription)\n",
    "    sentiment_label = score_to_label(sentiment_score)\n",
    "    \n",
    "    print(f\"Sentiment: {sentiment_label} (score: {sentiment_score:.2f})\")\n",
    "    \n",
    "    return {\n",
    "        \"audio_path\": audio_path,\n",
    "        \"transcription\": transcription,\n",
    "        \"sentiment_score\": sentiment_score,\n",
    "        \"sentiment_label\": sentiment_label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline on a sample\n",
    "sample_path = list(audio_paths.keys())[1]  # Happy sample\n",
    "result = analyze_speech_sentiment(sample_path)\n",
    "print(\"\n",
    "Result:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb97af",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "1. **Integrate with Text Model**: Replace the simple rule-based sentiment analyzer with your trained model from Part 1\n",
    "2. **Create a Web Interface**: Implement the Flask web app for uploading and analyzing audio\n",
    "3. **Support Longer Audio**: Add functionality to process longer audio files\n",
    "4. **Compare with LLMs**: In Part 3, you will compare this approach with LLM-based sentiment analysis"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
