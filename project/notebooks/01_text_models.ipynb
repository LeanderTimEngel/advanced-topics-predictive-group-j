{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Input, Embedding, Bidirectional, LSTM,\n",
    "    Conv1D, GlobalMaxPooling1D, Dense, Dropout, concatenate\n",
    ")\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 1: Load & Concatenate Data\n",
    "# -----------------------------\n",
    "paths = [\n",
    "    '../data/goemotions/goemotions_1.csv',\n",
    "    '../data/goemotions/goemotions_2.csv',\n",
    "    '../data/goemotions/goemotions_3.csv'\n",
    "]\n",
    "# Read and concatenate all parts\n",
    "dfs = [pd.read_csv(p) for p in paths]\n",
    "df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 2: Text Cleaning Function\n",
    "# -----------------------------\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Lowercase, remove HTML tags, strip non-alphanumeric characters,\n",
    "    and normalize whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)            # Remove HTML tags\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)      # Remove punctuation/special chars\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()     # Collapse whitespace\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df['clean_text'] = df['text'].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 3: Train/Val/Test Split\n",
    "# -----------------------------\n",
    "# 80% train, 10% validation, 10% test\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df  = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 4: Build Vocabulary & Tokenize\n",
    "# -----------------------------\n",
    "max_words = 20000    # Vocabulary size\n",
    "max_len   = 128      # Sequence length\n",
    "\n",
    "# Count word frequencies on training set\n",
    "counter = Counter()\n",
    "for text in train_df['clean_text']:\n",
    "    counter.update(text.split())\n",
    "\n",
    "# Keep top (max_words - 1) words; reserve 0=PAD, 1=OOV\n",
    "vocab = [w for w, _ in counter.most_common(max_words-1)]\n",
    "word_index = {w: i+2 for i, w in enumerate(vocab)}\n",
    "\n",
    "def text_to_seq(text):\n",
    "    \"\"\"Convert cleaned text into a list of integer token IDs.\"\"\"\n",
    "    return [word_index.get(w, 1) for w in text.split()]  # 1 = OOV\n",
    "\n",
    "def pad_seq(seq, max_len):\n",
    "    \"\"\"Truncate or pad sequence with zeros up to max_len.\"\"\"\n",
    "    if len(seq) > max_len:\n",
    "        return seq[:max_len]\n",
    "    return seq + [0] * (max_len - len(seq))\n",
    "\n",
    "# Convert and pad all splits\n",
    "train_seq = np.array([pad_seq(text_to_seq(t), max_len) for t in train_df['clean_text']], dtype=np.int16)\n",
    "val_seq   = np.array([pad_seq(text_to_seq(t), max_len) for t in val_df['clean_text']],   dtype=np.int16)\n",
    "test_seq  = np.array([pad_seq(text_to_seq(t), max_len) for t in test_df['clean_text']],  dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 5: Prepare Multi-Label Targets\n",
    "# -----------------------------\n",
    "# Identify emotion columns (exclude metadata fields)\n",
    "exclude = {'text','id','author','subreddit','link_id','parent_id','created_utc','rater_id','example_very_unclear'}\n",
    "label_cols = [c for c in df.columns if c not in exclude and c != 'clean_text']\n",
    "\n",
    "train_labels = train_df[label_cols].values\n",
    "val_labels   = val_df[label_cols].values\n",
    "test_labels  = test_df[label_cols].values\n",
    "\n",
    "num_classes = len(label_cols)\n",
    "vocab_size  = max_words + 2  # accounting for PAD and OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 6: Custom Callback for Macro-F1\n",
    "# -----------------------------\n",
    "class F1Metrics(Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.val_seq, self.val_labels = validation_data\n",
    "        self.val_f1s = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Predict and binarize at threshold 0.5\n",
    "        preds = (self.model.predict(self.val_seq) > 0.5).astype(int)\n",
    "        # Compute macro-F1\n",
    "        f1 = f1_score(self.val_labels, preds, average='macro')\n",
    "        self.val_f1s.append(f1)\n",
    "        print(f\" â€” val_macro_f1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 7: Define Model Architectures\n",
    "# -----------------------------\n",
    "def build_lstm_model(vocab_size, max_len, num_classes,\n",
    "                     emb_dim=100, lstm_units=128):\n",
    "    \"\"\"\n",
    "    Bi-directional LSTM model:\n",
    "    Embedding -> Bi-LSTM -> Dropout -> Dense(sigmoid)\n",
    "    \"\"\"\n",
    "    inp = Input(shape=(max_len,))\n",
    "    x   = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=max_len)(inp)\n",
    "    x   = Bidirectional(LSTM(lstm_units))(x)\n",
    "    x   = Dropout(0.5)(x)\n",
    "    out = Dense(num_classes, activation='sigmoid')(x)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(1e-3),\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_cnn_model(vocab_size, max_len, num_classes,\n",
    "                    emb_dim=100, filters=64, kernel_sizes=[3,4,5]):\n",
    "    \"\"\"\n",
    "    Multi-filter 1D-CNN:\n",
    "    Embedding -> Conv1D(each size) -> GlobalMaxPool -> Concat -> Dropout -> Dense(sigmoid)\n",
    "    \"\"\"\n",
    "    inp = Input(shape=(max_len,))\n",
    "    x   = Embedding(vocab_size, emb_dim, input_length=max_len)(inp)\n",
    "    convs = []\n",
    "    for k in kernel_sizes:\n",
    "        c = Conv1D(filters, k, activation='relu')(x)\n",
    "        c = GlobalMaxPooling1D()(c)\n",
    "        convs.append(c)\n",
    "    x   = concatenate(convs)\n",
    "    x   = Dropout(0.5)(x)\n",
    "    out = Dense(num_classes, activation='sigmoid')(x)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(1e-3),\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leander/Desktop/advanced pa/advanced-topics-predictive-group-j/venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Step 8: Train Models\n",
    "# -----------------------------\n",
    "batch_size = 32\n",
    "epochs     = 5  # increase for better convergence\n",
    "\n",
    "# Bi-LSTM training\n",
    "lstm_model = build_lstm_model(vocab_size, max_len, num_classes)\n",
    "f1_cb_lstm = F1Metrics((val_seq, val_labels))\n",
    "history_lstm = lstm_model.fit(\n",
    "    train_seq, train_labels,\n",
    "    validation_data=(val_seq, val_labels),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[f1_cb_lstm],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 1D-CNN training\n",
    "cnn_model = build_cnn_model(vocab_size, max_len, num_classes)\n",
    "f1_cb_cnn = F1Metrics((val_seq, val_labels))\n",
    "history_cnn = cnn_model.fit(\n",
    "    train_seq, train_labels,\n",
    "    validation_data=(val_seq, val_labels),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[f1_cb_cnn],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 9: Plot Training Curves\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Plot loss curves\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history_lstm.history['loss'],   label='LSTM Train Loss')\n",
    "plt.plot(history_lstm.history['val_loss'], label='LSTM Val Loss')\n",
    "plt.plot(history_cnn.history['loss'],    label='CNN Train Loss')\n",
    "plt.plot(history_cnn.history['val_loss'],   label='CNN Val Loss')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Plot validation Macro-F1\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(f1_cb_lstm.val_f1s, label='LSTM Val Macro-F1')\n",
    "plt.plot(f1_cb_cnn.val_f1s,   label='CNN Val Macro-F1')\n",
    "plt.title('Validation Macro-F1 per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 10: Final Evaluation on Test Set\n",
    "# -----------------------------\n",
    "for name, model in [('Bi-LSTM', lstm_model), ('1D-CNN', cnn_model)]:\n",
    "    preds = (model.predict(test_seq) > 0.5).astype(int)\n",
    "    macro_f1 = f1_score(test_labels, preds, average='macro')\n",
    "    micro_f1 = f1_score(test_labels, preds, average='micro')\n",
    "    p, r, f, _ = precision_recall_fscore_support(\n",
    "        test_labels, preds, average=None, zero_division=0\n",
    "    )\n",
    "    print(f\"\\n{name} â€” Test Macro-F1: {macro_f1:.4f}, Micro-F1: {micro_f1:.4f}\")\n",
    "    # Display metrics for first 5 emotions\n",
    "    print(\"Sample per-emotion (first 5):\")\n",
    "    for i in range(5):\n",
    "        print(f\"  {label_cols[i]}: P={p[i]:.2f}, R={r[i]:.2f}, F1={f[i]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
