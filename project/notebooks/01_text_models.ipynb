{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────── 0. Imports ───────────────────────────────\n",
    "import re, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, classification_report, confusion_matrix, precision_recall_curve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import (\n",
    "    Input, Embedding, Dense, Dropout, LayerNormalization, Add,\n",
    "    Bidirectional, LSTM,\n",
    "    Conv1D, GlobalMaxPooling1D, concatenate,\n",
    "    MultiHeadAttention, GlobalAveragePooling1D,\n",
    ")\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────── 1. Hyper-params ────────────────────────────\n",
    "MAX_WORDS   = 20_000       # size of the word vocabulary\n",
    "MAX_LEN     = 32           # sequence length after padding / truncation\n",
    "BATCH_SIZE  = 32\n",
    "EPOCHS      = 20\n",
    "PATIENCE_ES = 3            # EarlyStopping patience\n",
    "PATIENCE_LR = 2            # LR-scheduler patience\n",
    "SEED        = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────── 2. Load & Clean ───────────────────────────\n",
    "DATA_DIR = Path('../data/goemotions')\n",
    "\n",
    "df = pd.concat([pd.read_csv(p) for p in DATA_DIR.glob('goemotions_*.csv')],\n",
    "               ignore_index=True)\n",
    "\n",
    "def basic_clean(text: str) -> str:\n",
    "    \"\"\"Lower-case, drop HTML, punctuation, duplicate spaces.\"\"\"\n",
    "    text = re.sub(r'<.*?>', '', text.lower())          # remove HTML\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)            # keep alphanum/space\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "df['clean_text'] = df['text'].astype(str).apply(basic_clean)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Make sure emotion columns are numeric (int8 saves RAM)\n",
    "# -------------------------------------------------------------------------\n",
    "META = {'text','id','author','subreddit','link_id','parent_id',\n",
    "        'created_utc','rater_id','example_very_unclear'}\n",
    "label_cols = [c for c in df.columns if c not in META and c != 'clean_text']\n",
    "df[label_cols] = df[label_cols].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────── 3. Train / Val / Test split ───────────────\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "val_df,   test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────── 4. Tokenisation ───────────────────────────\n",
    "# 4-a Build vocabulary on the training set\n",
    "counter = Counter(); [counter.update(t.split()) for t in train_df['clean_text']]\n",
    "vocab = [w for w,_ in counter.most_common(MAX_WORDS-1)]\n",
    "word2idx = {w:i+2 for i,w in enumerate(vocab)}  # 0=PAD,1=OOV\n",
    "VOCAB_SIZE = MAX_WORDS + 2\n",
    "\n",
    "def text_to_ids(txt): return [word2idx.get(tok,1) for tok in txt.split()]\n",
    "def pad32(seq): return (seq[:MAX_LEN] + [0]*MAX_LEN)[:MAX_LEN]\n",
    "def to_tensor(series): return np.array([pad32(text_to_ids(t)) for t in series],\n",
    "                                       dtype=np.int16)\n",
    "\n",
    "train_x = to_tensor(train_df['clean_text'])\n",
    "val_x   = to_tensor(val_df['clean_text'])\n",
    "test_x  = to_tensor(test_df['clean_text'])\n",
    "\n",
    "train_y = train_df[label_cols].values\n",
    "val_y   = val_df[label_cols].values\n",
    "test_y  = test_df[label_cols].values\n",
    "N_LABELS = len(label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived positive / negative flags.\n"
     ]
    }
   ],
   "source": [
    "# ─────────── 4-bis-prep. Derive positive / negative flags if missing ─────\n",
    "if 'positive' not in df.columns:\n",
    "    pos_emotions = ['admiration','amusement','approval','caring','desire',\n",
    "                    'excitement','gratitude','joy','love','optimism',\n",
    "                    'pride','relief']\n",
    "    neg_emotions = ['anger','annoyance','confusion','disappointment',\n",
    "                    'disapproval','disgust','embarrassment','fear','grief',\n",
    "                    'nervousness','remorse','sadness']\n",
    "    missing = set(pos_emotions + neg_emotions) - set(label_cols)\n",
    "    if missing:\n",
    "        raise ValueError(f'Missing emotion columns: {missing}')\n",
    "    df['positive'] = (df[pos_emotions].sum(axis=1) > 0).astype('int8')\n",
    "    df['negative'] = (df[neg_emotions].sum(axis=1) > 0).astype('int8')\n",
    "    print('Derived positive / negative flags.')\n",
    "else:\n",
    "    print('positive / negative already present.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building binary sentiment subset ===\n",
      "Train 104,800 samples  pos-ratio 0.58\n",
      "Epoch 1/6\n",
      "737/737 - 49s - 67ms/step - accuracy: 0.7819 - loss: 0.4504 - val_accuracy: 0.8282 - val_loss: 0.3885\n",
      "Epoch 2/6\n",
      "737/737 - 48s - 66ms/step - accuracy: 0.8477 - loss: 0.3513 - val_accuracy: 0.8342 - val_loss: 0.3895\n",
      "Epoch 3/6\n",
      "737/737 - 46s - 62ms/step - accuracy: 0.8701 - loss: 0.3056 - val_accuracy: 0.8339 - val_loss: 0.4118\n",
      "Binary sentiment accuracy: 0.823\n",
      "Saved -> sentiment_goemotions_cnn.keras\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ░░░░░░░░░░░ 4-bis. Binary Sentiment Dataset (GoEmotions) ░░░░░░░░░░░\n",
    "print('\\n=== Building binary sentiment subset ===')\n",
    "\n",
    "mask_bin = (df['positive']==1) ^ (df['negative']==1)\n",
    "sent_df  = df.loc[mask_bin, ['clean_text','positive']].copy()\n",
    "sent_df.rename(columns={'positive':'label'}, inplace=True)\n",
    "\n",
    "X_train_txt, X_test_txt, y_train_bin, y_test_bin = train_test_split(\n",
    "    sent_df['clean_text'], sent_df['label'],\n",
    "    test_size=0.2, random_state=SEED, stratify=sent_df['label']\n",
    ")\n",
    "X_train_bin = to_tensor(X_train_txt)\n",
    "X_test_bin  = to_tensor(X_test_txt)\n",
    "\n",
    "print(f'Train {len(X_train_bin):,} samples  pos-ratio {y_train_bin.mean():.2f}')\n",
    "\n",
    "def build_sentiment_cnn():\n",
    "    inp = Input((MAX_LEN,))\n",
    "    x   = Embedding(VOCAB_SIZE, 128)(inp)\n",
    "    x   = Conv1D(128, 5, activation='relu')(x)\n",
    "    x   = GlobalMaxPooling1D()(x)\n",
    "    x   = Dropout(0.5)(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    m   = keras.Model(inp, out)\n",
    "    m.compile(Adam(1e-3), 'binary_crossentropy', metrics=['accuracy'])\n",
    "    return m\n",
    "\n",
    "sent_cnn = build_sentiment_cnn()\n",
    "sent_cnn.fit(X_train_bin, y_train_bin,\n",
    "             validation_split=0.1,\n",
    "             epochs=6, batch_size=128,\n",
    "             callbacks=[keras.callbacks.EarlyStopping(patience=2,\n",
    "                                                      restore_best_weights=True)],\n",
    "             verbose=2)\n",
    "print(f'Binary sentiment accuracy: {sent_cnn.evaluate(X_test_bin, y_test_bin, verbose=0)[1]:.3f}')\n",
    "sent_cnn.save('sentiment_goemotions_cnn.keras')\n",
    "print('Saved -> sentiment_goemotions_cnn.keras\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────── 5. Class Weights ──────────────────────────\n",
    "pos = train_y.sum(axis=0)\n",
    "neg = len(train_y) - pos\n",
    "# inverse frequency; cap very rare labels to 25× weight\n",
    "class_weight = {i: float(np.clip(neg[i] / pos[i], 1.0, 25.0))\n",
    "                for i in range(N_LABELS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────── 6. Macro-F1 Callback ──────────────────────\n",
    "class MacroF1(Callback):\n",
    "    \"\"\"Compute macro-F1 on validation set after each epoch.\"\"\"\n",
    "    def __init__(self, val_data):\n",
    "        super().__init__()\n",
    "        self.val_x, self.val_y = val_data\n",
    "        self.history = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        preds = (self.model.predict(self.val_x, verbose=0) > 0.5).astype(int)\n",
    "        f1 = f1_score(self.val_y, preds, average='macro')\n",
    "        self.history.append(f1)\n",
    "        print(f' — val_macro_f1: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────── 7. Encoder Blocks ─────────────────────────\n",
    "def build_lstm(emb_dim=100, units=128):\n",
    "    inp = Input(shape=(MAX_LEN,))\n",
    "    x   = Embedding(VOCAB_SIZE, emb_dim)(inp)\n",
    "    x   = Bidirectional(LSTM(units))(x)\n",
    "    x   = Dropout(0.5)(x)\n",
    "    out = Dense(N_LABELS, activation='sigmoid')(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(Adam(1e-3), 'binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_cnn(emb_dim=100, filters=64, kernel_sizes=(3,4,5)):\n",
    "    inp = Input(shape=(MAX_LEN,))\n",
    "    x   = Embedding(VOCAB_SIZE, emb_dim)(inp)\n",
    "    pooled = [GlobalMaxPooling1D()(Conv1D(filters, k, activation='relu')(x))\n",
    "              for k in kernel_sizes]\n",
    "    x   = concatenate(pooled)\n",
    "    x   = Dropout(0.5)(x)\n",
    "    out = Dense(N_LABELS, activation='sigmoid')(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(Adam(1e-3), 'binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "# ---- tiny Transformer encoder ------------------------------------------\n",
    "def tfm_block(x, heads=4, proj=64, mlp_dim=128):\n",
    "    \"\"\"One transformer encoder block with residual connections.\"\"\"\n",
    "    attn_out = MultiHeadAttention(num_heads=heads, key_dim=proj)(x, x)\n",
    "    x = LayerNormalization()(Add()([x, attn_out]))\n",
    "\n",
    "    mlp_out = Dense(mlp_dim, activation='relu')(x)\n",
    "    mlp_out = Dense(x.shape[-1])(mlp_out)\n",
    "    return LayerNormalization()(Add()([x, mlp_out]))\n",
    "\n",
    "def build_transformer(emb_dim=128, blocks=2, heads=4):\n",
    "    inp = Input(shape=(MAX_LEN,))\n",
    "    tok_emb = Embedding(VOCAB_SIZE, emb_dim)(inp)\n",
    "    # learnable positional embedding\n",
    "    pos_emb = Embedding(MAX_LEN, emb_dim)(tf.range(MAX_LEN)[None, :])\n",
    "    x = tok_emb + pos_emb\n",
    "\n",
    "    for _ in range(blocks):\n",
    "        x = tfm_block(x, heads, emb_dim // heads, emb_dim * 2)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    out = Dense(N_LABELS, activation='sigmoid')(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(Adam(1e-3), 'binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    return model\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# Common callbacks\n",
    "CALLBACKS = [\n",
    "    keras.callbacks.EarlyStopping(patience=PATIENCE_ES, restore_best_weights=True,\n",
    "                                  min_delta=1e-4),\n",
    "    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=PATIENCE_LR,\n",
    "                                      min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "def train_model(build_fn, name):\n",
    "    \"\"\"Utility: build, fit, return trained model + F1 curve.\"\"\"\n",
    "    K.clear_session()                                   # free GPU/CPU memory\n",
    "    model = build_fn()\n",
    "    f1_cb = MacroF1((val_x, val_y))\n",
    "    model.fit(train_x, train_y,\n",
    "              validation_data=(val_x, val_y),\n",
    "              epochs=EPOCHS,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              class_weight=class_weight,\n",
    "              callbacks=[f1_cb, *CALLBACKS],\n",
    "              verbose=2)\n",
    "    return model, f1_cb.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────── 7.1 Hyperparameter Tuning ────────────────────────\n",
    "def hyperparameter_search_cnn_efficient():\n",
    "    \"\"\"More efficient grid search for CNN hyperparameters.\"\"\"\n",
    "    print(\"\\n=== CNN Hyperparameter Tuning (Efficient) ===\")\n",
    "    best_f1 = 0\n",
    "    best_params = {}\n",
    "    results = []\n",
    "    \n",
    "    # Check if GPU is available\n",
    "    gpu_available = len(tf.config.list_physical_devices('GPU')) > 0\n",
    "    print(f\"GPU available: {gpu_available}\")\n",
    "    \n",
    "    # Smaller parameter grid\n",
    "    param_grid = {\n",
    "        'emb_dim': [100],          # Reduced from [50, 100, 200]\n",
    "        'filters': [64, 128],       # Reduced from [32, 64, 128]\n",
    "        'dropout': [0.5]            # Reduced from [0.3, 0.5, 0.7]\n",
    "    }\n",
    "    \n",
    "    # Create a smaller subset for faster tuning\n",
    "    sample_size = min(5000, len(train_x))\n",
    "    indices = np.random.choice(len(train_x), sample_size, replace=False)\n",
    "    train_x_sample = train_x[indices]\n",
    "    train_y_sample = train_y[indices]\n",
    "    \n",
    "    print(f\"Using {sample_size} samples for tuning (from {len(train_x)} total)\")\n",
    "    \n",
    "    # Display total combinations\n",
    "    total_combinations = len(param_grid['emb_dim']) * len(param_grid['filters']) * len(param_grid['dropout'])\n",
    "    print(f\"Testing {total_combinations} parameter combinations\")\n",
    "    \n",
    "    for emb_dim in param_grid['emb_dim']:\n",
    "        for filters in param_grid['filters']:\n",
    "            for dropout in param_grid['dropout']:\n",
    "                params = {\n",
    "                    'emb_dim': emb_dim,\n",
    "                    'filters': filters,\n",
    "                    'dropout': dropout\n",
    "                }\n",
    "                print(f\"\\nTesting: emb_dim={emb_dim}, filters={filters}, dropout={dropout}\")\n",
    "                \n",
    "                # Define model with these hyperparameters\n",
    "                def build_model():\n",
    "                    inp = Input(shape=(MAX_LEN,))\n",
    "                    x = Embedding(VOCAB_SIZE, emb_dim)(inp)\n",
    "                    pooled = [GlobalMaxPooling1D()(Conv1D(filters, k, activation='relu')(x))\n",
    "                             for k in [3, 4, 5]]\n",
    "                    x = concatenate(pooled)\n",
    "                    x = Dropout(dropout)(x)\n",
    "                    out = Dense(N_LABELS, activation='sigmoid')(x)\n",
    "                    model = keras.Model(inp, out)\n",
    "                    model.compile(Adam(1e-3), 'binary_crossentropy', metrics=['binary_accuracy'])\n",
    "                    return model\n",
    "                \n",
    "                # Train with fewer epochs for hyperparameter search\n",
    "                K.clear_session()\n",
    "                model = build_model()\n",
    "                f1_cb = MacroF1((val_x, val_y))\n",
    "                \n",
    "                history = model.fit(\n",
    "                    train_x_sample, train_y_sample,  # Using sampled data\n",
    "                    validation_data=(val_x, val_y),\n",
    "                    epochs=3,  # Reduced epochs for faster tuning\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=[f1_cb],  # Reduced callbacks\n",
    "                    class_weight=class_weight,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Get best F1 score\n",
    "                val_f1 = max(f1_cb.history)\n",
    "                \n",
    "                # Track results\n",
    "                results.append({**params, 'val_f1': val_f1})\n",
    "                \n",
    "                if val_f1 > best_f1:\n",
    "                    best_f1 = val_f1\n",
    "                    best_params = params\n",
    "                \n",
    "                # Free memory\n",
    "                K.clear_session()\n",
    "    \n",
    "    # Convert results to DataFrame for easier analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"\\nBest parameters: {best_params}, F1: {best_f1:.4f}\")\n",
    "    print(\"\\nAll configurations:\")\n",
    "    print(results_df.sort_values('val_f1', ascending=False).to_string(index=False))\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────── 7.2 Cross-Validation ────────────────────────\n",
    "def cross_validate_model_efficient(build_fn, name, k=3):\n",
    "    \"\"\"Perform k-fold cross-validation on the model with efficiency optimizations.\"\"\"\n",
    "    print(f\"\\n=== {name} {k}-Fold Cross-Validation (Efficient) ===\")\n",
    "    \n",
    "    # Create k folds with fewer splits\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # Combine train and validation data for CV\n",
    "    combined_x = np.concatenate([train_x, val_x])\n",
    "    combined_y = np.concatenate([train_y, val_y])\n",
    "    \n",
    "    # Take a sample to speed up cross-validation\n",
    "    sample_size = min(10000, len(combined_x))\n",
    "    indices = np.random.choice(len(combined_x), sample_size, replace=False)\n",
    "    combined_x_sample = combined_x[indices]\n",
    "    combined_y_sample = combined_y[indices]\n",
    "    \n",
    "    print(f\"Using {sample_size} samples for CV (from {len(combined_x)} total)\")\n",
    "    \n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(combined_x_sample)):\n",
    "        print(f\"\\nTraining fold {fold+1}/{k}\")\n",
    "        \n",
    "        # Get fold data\n",
    "        fold_train_x = combined_x_sample[train_idx]\n",
    "        fold_train_y = combined_y_sample[train_idx]\n",
    "        fold_val_x = combined_x_sample[val_idx]\n",
    "        fold_val_y = combined_y_sample[val_idx]\n",
    "        \n",
    "        # Build and train model\n",
    "        K.clear_session()\n",
    "        model = build_fn()\n",
    "        f1_cb = MacroF1((fold_val_x, fold_val_y))\n",
    "        \n",
    "        model.fit(\n",
    "            fold_train_x, fold_train_y,\n",
    "            validation_data=(fold_val_x, fold_val_y),\n",
    "            epochs=5,  # Further reduced epochs\n",
    "            batch_size=BATCH_SIZE * 2,  # Larger batch size for speed\n",
    "            callbacks=[f1_cb],  # Minimal callbacks\n",
    "            class_weight=class_weight,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Get best F1 score\n",
    "        best_f1 = max(f1_cb.history)\n",
    "        fold_scores.append(best_f1)\n",
    "        \n",
    "        print(f\"Fold {fold+1} best F1: {best_f1:.4f}\")\n",
    "    \n",
    "    # Calculate aggregate statistics\n",
    "    mean_f1 = np.mean(fold_scores)\n",
    "    std_f1 = np.std(fold_scores)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{name} Cross-validation results:\")\n",
    "    print(f\"F1 scores: {[f'{x:.4f}' for x in fold_scores]}\")\n",
    "    print(f\"Mean F1: {mean_f1:.4f}, Std: {std_f1:.4f}\")\n",
    "    \n",
    "    return {'f1_scores': fold_scores, 'mean_f1': mean_f1, 'std_f1': std_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " — val_macro_f1: 0.1276\n",
      "5281/5281 - 332s - 63ms/step - binary_accuracy: 0.9590 - loss: 2.3058 - val_binary_accuracy: 0.9600 - val_loss: 0.1357 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      " — val_macro_f1: 0.1786\n",
      "5281/5281 - 367s - 70ms/step - binary_accuracy: 0.9598 - loss: 1.9936 - val_binary_accuracy: 0.9601 - val_loss: 0.1316 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      " — val_macro_f1: 0.2021\n",
      "5281/5281 - 454s - 86ms/step - binary_accuracy: 0.9601 - loss: 1.8989 - val_binary_accuracy: 0.9602 - val_loss: 0.1303 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      " — val_macro_f1: 0.2195\n",
      "5281/5281 - 297s - 56ms/step - binary_accuracy: 0.9603 - loss: 1.8338 - val_binary_accuracy: 0.9600 - val_loss: 0.1308 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      " — val_macro_f1: 0.2279\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "5281/5281 - 383s - 73ms/step - binary_accuracy: 0.9605 - loss: 1.7816 - val_binary_accuracy: 0.9599 - val_loss: 0.1311 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      " — val_macro_f1: 0.2456\n",
      "5281/5281 - 359s - 68ms/step - binary_accuracy: 0.9610 - loss: 1.6940 - val_binary_accuracy: 0.9592 - val_loss: 0.1320 - learning_rate: 5.0000e-04\n",
      "Epoch 1/20\n",
      " — val_macro_f1: 0.1677\n",
      "5281/5281 - 158s - 30ms/step - binary_accuracy: 0.9588 - loss: 2.3110 - val_binary_accuracy: 0.9604 - val_loss: 0.1369 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      " — val_macro_f1: 0.1884\n",
      "5281/5281 - 156s - 30ms/step - binary_accuracy: 0.9595 - loss: 2.0702 - val_binary_accuracy: 0.9604 - val_loss: 0.1336 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      " — val_macro_f1: 0.2200\n",
      "5281/5281 - 158s - 30ms/step - binary_accuracy: 0.9596 - loss: 1.9948 - val_binary_accuracy: 0.9600 - val_loss: 0.1347 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      " — val_macro_f1: 0.2174\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "5281/5281 - 153s - 29ms/step - binary_accuracy: 0.9597 - loss: 1.9348 - val_binary_accuracy: 0.9600 - val_loss: 0.1338 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      " — val_macro_f1: 0.2294\n",
      "5281/5281 - 1271s - 241ms/step - binary_accuracy: 0.9603 - loss: 1.8352 - val_binary_accuracy: 0.9598 - val_loss: 0.1332 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      " — val_macro_f1: 0.2316\n",
      "5281/5281 - 155s - 29ms/step - binary_accuracy: 0.9604 - loss: 1.7950 - val_binary_accuracy: 0.9596 - val_loss: 0.1337 - learning_rate: 5.0000e-04\n",
      "Epoch 7/20\n",
      " — val_macro_f1: 0.2321\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "5281/5281 - 155s - 29ms/step - binary_accuracy: 0.9605 - loss: 1.7656 - val_binary_accuracy: 0.9595 - val_loss: 0.1344 - learning_rate: 5.0000e-04\n",
      "Epoch 8/20\n",
      " — val_macro_f1: 0.2439\n",
      "5281/5281 - 155s - 29ms/step - binary_accuracy: 0.9610 - loss: 1.7116 - val_binary_accuracy: 0.9594 - val_loss: 0.1343 - learning_rate: 2.5000e-04\n",
      "Epoch 1/20\n",
      " — val_macro_f1: 0.1578\n",
      "5281/5281 - 545s - 103ms/step - binary_accuracy: 0.9591 - loss: 2.2310 - val_binary_accuracy: 0.9601 - val_loss: 0.1354 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      " — val_macro_f1: 0.1930\n",
      "5281/5281 - 529s - 100ms/step - binary_accuracy: 0.9597 - loss: 1.9893 - val_binary_accuracy: 0.9597 - val_loss: 0.1349 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      " — val_macro_f1: 0.1948\n",
      "5281/5281 - 4804s - 910ms/step - binary_accuracy: 0.9600 - loss: 1.8841 - val_binary_accuracy: 0.9593 - val_loss: 0.1368 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      " — val_macro_f1: 0.2113\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "5281/5281 - 1184s - 224ms/step - binary_accuracy: 0.9605 - loss: 1.7898 - val_binary_accuracy: 0.9584 - val_loss: 0.1397 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      " — val_macro_f1: 0.2426\n",
      "5281/5281 - 1511s - 286ms/step - binary_accuracy: 0.9615 - loss: 1.6464 - val_binary_accuracy: 0.9568 - val_loss: 0.1437 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────── 8. Train all three encoders ───────────────\n",
    "lstm_model, hist_lstm = train_model(build_lstm, 'LSTM')\n",
    "cnn_model,  hist_cnn  = train_model(build_cnn,  'CNN')\n",
    "tfm_model,  hist_tfm  = train_model(build_transformer, 'TFM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'precision_recall_curve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m opt_thr  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(N_LABELS)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_LABELS):\n\u001b[0;32m----> 7\u001b[0m     precision, recall, thresh \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_curve\u001b[49m(val_y[:, i], val_prob[:, i])\n\u001b[1;32m      8\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m precision \u001b[38;5;241m*\u001b[39m recall \u001b[38;5;241m/\u001b[39m (precision \u001b[38;5;241m+\u001b[39m recall \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[1;32m      9\u001b[0m     opt_thr[i] \u001b[38;5;241m=\u001b[39m thresh[f1\u001b[38;5;241m.\u001b[39margmax()]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'precision_recall_curve' is not defined"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────── 9. Per-class threshold tuning ─────────────\n",
    "# Use Transformer as best model; you can reuse this function for others.\n",
    "val_prob = tfm_model.predict(val_x, batch_size=256, verbose=0)\n",
    "opt_thr  = np.zeros(N_LABELS)\n",
    "\n",
    "for i in range(N_LABELS):\n",
    "    precision, recall, thresh = precision_recall_curve(val_y[:, i], val_prob[:, i])\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    opt_thr[i] = thresh[f1.argmax()]\n",
    "\n",
    "np.save('opt_thresholds.npy', opt_thr)   #  -> used later in Part 2/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────── 10. Final Test metrics ────────────────────\n",
    "def evaluate(model, name, thr=None):\n",
    "    prob = model.predict(test_x, batch_size=256, verbose=0)\n",
    "    pred = (prob >= thr) if thr is not None else (prob > 0.5)\n",
    "    pred = pred.astype(int)\n",
    "    macro = f1_score(test_y, pred, average='macro')\n",
    "    micro = f1_score(test_y, pred, average='micro')\n",
    "    print(f'{name:11s} | Macro-F1 {macro:.4f} | Micro-F1 {micro:.4f}')\n",
    "\n",
    "print('\\n── Test set with fixed 0.5 threshold ──')\n",
    "evaluate(lstm_model, 'Bi-LSTM')\n",
    "evaluate(cnn_model,  '1D-CNN')\n",
    "\n",
    "print('\\n── Test set with tuned thresholds (Transformer) ──')\n",
    "evaluate(tfm_model, 'Transformer', opt_thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────── 11. F1 curves plot ────────────────────────\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(hist_lstm, label='LSTM')\n",
    "plt.plot(hist_cnn,  label='CNN')\n",
    "plt.plot(hist_tfm,  label='Transformer')\n",
    "plt.ylabel('Validation Macro-F1'); plt.xlabel('Epoch'); plt.legend(); plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────── 12. Comprehensive Model Evaluation ─────────────────────────────\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, roc_curve, auc, classification_report\n",
    "\n",
    "def evaluate_model_comprehensive(model, x_test, y_test, label_cols, thresholds=None, name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation function that provides detailed performance analysis.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        x_test: Test features\n",
    "        y_test: Test labels\n",
    "        label_cols: List of emotion class names\n",
    "        thresholds: Optional per-class thresholds (default 0.5 if None)\n",
    "        name: Model name for display purposes\n",
    "    \n",
    "    Returns:\n",
    "        metrics_df: DataFrame with precision, recall and F1 for each class\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred_prob = model.predict(x_test, batch_size=256, verbose=0)\n",
    "    \n",
    "    # Apply thresholds (either per-class or global 0.5)\n",
    "    if thresholds is not None:\n",
    "        y_pred = np.zeros_like(y_pred_prob, dtype=int)\n",
    "        for i in range(y_pred_prob.shape[1]):\n",
    "            y_pred[:, i] = (y_pred_prob[:, i] >= thresholds[i]).astype(int)\n",
    "    else:\n",
    "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Get per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Calculate macro and micro averages\n",
    "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='micro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Emotion': label_cols,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall, \n",
    "        'F1-Score': f1,\n",
    "        'Support': support,\n",
    "        'Threshold': thresholds if thresholds is not None else [0.5] * len(label_cols)\n",
    "    })\n",
    "    \n",
    "    # Sort by F1 score (descending)\n",
    "    metrics_df = metrics_df.sort_values('F1-Score', ascending=False)\n",
    "    \n",
    "    # Print overview report\n",
    "    print(f\"\\n{'='*20} {name} Evaluation {'='*20}\")\n",
    "    print(f\"Macro-Precision: {macro_precision:.4f} | Macro-Recall: {macro_recall:.4f} | Macro-F1: {macro_f1:.4f}\")\n",
    "    print(f\"Micro-Precision: {micro_precision:.4f} | Micro-Recall: {micro_recall:.4f} | Micro-F1: {micro_f1:.4f}\")\n",
    "    \n",
    "    # Display top 5 and bottom 5 emotions by F1 score\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.precision', 4)\n",
    "    print(\"\\nTop 5 classes:\")\n",
    "    print(metrics_df.head(5).to_string(index=False))\n",
    "    print(\"\\nBottom 5 classes:\")\n",
    "    print(metrics_df.tail(5).to_string(index=False))\n",
    "    \n",
    "    # Calculate class distribution to show imbalance\n",
    "    class_dist = y_test.sum(axis=0) / len(y_test)\n",
    "    metrics_df['Class_Distribution'] = [class_dist[i] for i in range(len(label_cols))]\n",
    "    \n",
    "    # Save results\n",
    "    metrics_df.to_csv(f'{name.lower()}_evaluation_metrics.csv', index=False)\n",
    "    print(f\"Metrics saved to {name.lower()}_evaluation_metrics.csv\")\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "def plot_metrics_by_emotion(metrics_df, title=\"Model Performance by Emotion\", n_emotions=10):\n",
    "    \"\"\"\n",
    "    Plot the precision, recall, and F1 scores for top emotions.\n",
    "    \n",
    "    Args:\n",
    "        metrics_df: DataFrame with metrics (from evaluate_model_comprehensive)\n",
    "        title: Plot title\n",
    "        n_emotions: Number of top emotions to display\n",
    "    \"\"\"\n",
    "    # Get top emotions by F1 score for visualization\n",
    "    plot_df = metrics_df.head(n_emotions).copy()\n",
    "    \n",
    "    # Melt the DataFrame for easier plotting\n",
    "    plot_df_melted = pd.melt(\n",
    "        plot_df, \n",
    "        id_vars=['Emotion', 'Support', 'Threshold'],\n",
    "        value_vars=['Precision', 'Recall', 'F1-Score'],\n",
    "        var_name='Metric', \n",
    "        value_name='Score'\n",
    "    )\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Bar plot for metrics\n",
    "    g = sns.barplot(x='Emotion', y='Score', hue='Metric', data=plot_df_melted)\n",
    "    \n",
    "    # Add threshold as text on top of the bars\n",
    "    for i, emotion in enumerate(plot_df['Emotion']):\n",
    "        threshold = plot_df.loc[plot_df['Emotion'] == emotion, 'Threshold'].values[0]\n",
    "        support = plot_df.loc[plot_df['Emotion'] == emotion, 'Support'].values[0]\n",
    "        plt.text(i, 1.05, f'thr={threshold:.2f}\\nn={support}', ha='center')\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 1.15)  # Leave space for the annotations\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Metric')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(f\"{title.lower().replace(' ', '_')}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix_for_class(model, x_test, y_test, class_idx, class_name, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Creates and plots a confusion matrix for a specific class.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        x_test: Test features\n",
    "        y_test: Test labels\n",
    "        class_idx: Index of the class to analyze\n",
    "        class_name: Name of the class\n",
    "        threshold: Classification threshold\n",
    "    \"\"\"\n",
    "    # Get predictions for this specific class\n",
    "    y_pred_prob = model.predict(x_test, batch_size=256, verbose=0)[:, class_idx]\n",
    "    y_pred_class = (y_pred_prob >= threshold).astype(int)\n",
    "    y_true_class = y_test[:, class_idx]\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true_class, y_pred_class)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix for \"{class_name}\"')\n",
    "    \n",
    "    # Calculate metrics for this class\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nDetailed metrics for class '{class_name}':\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"True Positives: {tp}, False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}, True Negatives: {tn}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"confusion_matrix_{class_name.lower()}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "def plot_roc_curves(model, x_test, y_test, label_cols, top_n=5):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for the top n classes with highest AUC.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        x_test: Test features\n",
    "        y_test: Test labels\n",
    "        label_cols: List of emotion class names\n",
    "        top_n: Number of top classes to plot\n",
    "    \"\"\"\n",
    "    y_pred_prob = model.predict(x_test, batch_size=256, verbose=0)\n",
    "    \n",
    "    # Calculate AUC for each class\n",
    "    auc_scores = []\n",
    "    for i in range(len(label_cols)):\n",
    "        fpr, tpr, _ = roc_curve(y_test[:, i], y_pred_prob[:, i])\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        auc_scores.append((label_cols[i], auc_score, fpr, tpr))\n",
    "    \n",
    "    # Sort by AUC\n",
    "    auc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Plot top n\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(min(top_n, len(auc_scores))):\n",
    "        emotion, auc_score, fpr, tpr = auc_scores[i]\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'{emotion} (AUC = {auc_score:.4f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves for Top {top_n} Emotions')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"roc_curves_top{top_n}.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return auc_scores\n",
    "\n",
    "def plot_model_comparison(models_metrics, title=\"Model Comparison\"):\n",
    "    \"\"\"\n",
    "    Compare F1 scores across different models.\n",
    "    \n",
    "    Args:\n",
    "        models_metrics: List of tuples (model_name, metrics_df)\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Get common emotions in all models (top 10)\n",
    "    common_emotions = set(models_metrics[0][1]['Emotion'].head(10))\n",
    "    for _, df in models_metrics[1:]:\n",
    "        common_emotions &= set(df['Emotion'].head(10))\n",
    "    common_emotions = list(common_emotions)[:10]  # Limit to top 10\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    model_names = [name for name, _ in models_metrics]\n",
    "    emotion_f1_scores = {}\n",
    "    \n",
    "    for emotion in common_emotions:\n",
    "        emotion_f1_scores[emotion] = []\n",
    "        for _, df in models_metrics:\n",
    "            f1 = df.loc[df['Emotion'] == emotion, 'F1-Score'].values[0]\n",
    "            emotion_f1_scores[emotion].append(f1)\n",
    "    \n",
    "    # Plot\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.8 / len(common_emotions)\n",
    "    \n",
    "    for i, emotion in enumerate(common_emotions):\n",
    "        plt.bar(x + i*width - 0.4 + width/2, emotion_f1_scores[emotion], \n",
    "                width=width, label=emotion)\n",
    "    \n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xticks(x, model_names)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend(title='Emotion', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{title.lower().replace(' ', '_')}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def analyze_error_patterns(model, x_test, y_test, label_cols, thresholds=None):\n",
    "    \"\"\"\n",
    "    Analyze error patterns in the model predictions.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        x_test: Test features\n",
    "        y_test: Test labels\n",
    "        label_cols: List of emotion class names\n",
    "        thresholds: Optional per-class thresholds\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred_prob = model.predict(x_test, batch_size=256, verbose=0)\n",
    "    \n",
    "    # Apply thresholds\n",
    "    if thresholds is not None:\n",
    "        y_pred = np.zeros_like(y_pred_prob, dtype=int)\n",
    "        for i in range(y_pred_prob.shape[1]):\n",
    "            y_pred[:, i] = (y_pred_prob[:, i] >= thresholds[i]).astype(int)\n",
    "    else:\n",
    "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Identify common error patterns\n",
    "    # 1. False positives and false negatives per class\n",
    "    fp_counts = ((y_pred == 1) & (y_test == 0)).sum(axis=0)\n",
    "    fn_counts = ((y_pred == 0) & (y_test == 1)).sum(axis=0)\n",
    "    \n",
    "    # 2. Co-occurrence matrix of errors\n",
    "    error_indices = np.where(y_pred != y_test)\n",
    "    error_pairs = []\n",
    "    \n",
    "    for i in range(len(error_indices[0])):\n",
    "        sample_idx = error_indices[0][i]\n",
    "        class_idx = error_indices[1][i]\n",
    "        \n",
    "        # For each error, find other errors in the same sample\n",
    "        for other_class_idx in range(y_test.shape[1]):\n",
    "            if other_class_idx != class_idx and y_pred[sample_idx, other_class_idx] != y_test[sample_idx, other_class_idx]:\n",
    "                error_pairs.append((class_idx, other_class_idx))\n",
    "    \n",
    "    # Count error co-occurrences\n",
    "    from collections import Counter\n",
    "    error_cooccur = Counter(error_pairs)\n",
    "    \n",
    "    # Prepare results\n",
    "    error_df = pd.DataFrame({\n",
    "        'Emotion': label_cols,\n",
    "        'False Positives': fp_counts,\n",
    "        'False Negatives': fn_counts,\n",
    "        'FP Rate': fp_counts / np.maximum((y_test == 0).sum(axis=0), 1),\n",
    "        'FN Rate': fn_counts / np.maximum((y_test == 1).sum(axis=0), 1)\n",
    "    })\n",
    "    \n",
    "    # Sort by total errors\n",
    "    error_df['Total Errors'] = error_df['False Positives'] + error_df['False Negatives']\n",
    "    error_df = error_df.sort_values('Total Errors', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 5 classes with most errors:\")\n",
    "    print(error_df.head(5).to_string(index=False))\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    error_df_plot = error_df.head(10).copy()  # Top 10 classes with errors\n",
    "    \n",
    "    x = np.arange(len(error_df_plot))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, error_df_plot['False Positives'], width, label='False Positives')\n",
    "    plt.bar(x + width/2, error_df_plot['False Negatives'], width, label='False Negatives')\n",
    "    \n",
    "    plt.xlabel('Emotion')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Error Distribution by Emotion Class')\n",
    "    plt.xticks(x, error_df_plot['Emotion'], rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"error_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────── 13. Run Evaluation ─────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment the following lines to run the hyperparameter tuning and cross-validation\n",
    "    \n",
    "    # Run hyperparameter tuning for CNN model\n",
    "    print(\"\\nRunning CNN hyperparameter tuning...\")\n",
    "    # best_cnn_params = hyperparameter_search_cnn_efficient()  # Use efficient version for faster execution\n",
    "    \n",
    "    # Now define a build function with the optimal parameters\n",
    "    # def build_optimal_cnn():\n",
    "    #     return build_cnn(\n",
    "    #         emb_dim=best_cnn_params['emb_dim'],\n",
    "    #         filters=best_cnn_params['filters'],\n",
    "    #         kernel_sizes=(3, 4, 5)\n",
    "    #     )\n",
    "    \n",
    "    # Run cross-validation on models\n",
    "    print(\"\\nRunning cross-validation on models...\")\n",
    "    # cv_lstm_results = cross_validate_model_efficient(build_lstm, \"BiLSTM\")\n",
    "    # cv_cnn_results = cross_validate_model_efficient(build_optimal_cnn if 'best_cnn_params' in locals() else build_cnn, \"CNN\")\n",
    "    \n",
    "    # Train the final models if not already trained\n",
    "    print(\"\\nTraining final models...\")\n",
    "    if 'lstm_model' not in globals():\n",
    "        lstm_model, hist_lstm = train_model(build_lstm, 'LSTM')\n",
    "    if 'cnn_model' not in globals():\n",
    "        cnn_model, hist_cnn = train_model(build_cnn, 'CNN')\n",
    "    if 'tfm_model' not in globals():\n",
    "        tfm_model, hist_tfm = train_model(build_transformer, 'TFM')\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    print(\"\\nRunning comprehensive evaluation...\")\n",
    "    lstm_metrics = evaluate_model_comprehensive(lstm_model, test_x, test_y, label_cols, name=\"BiLSTM\")\n",
    "    cnn_metrics = evaluate_model_comprehensive(cnn_model, test_x, test_y, label_cols, name=\"CNN\")\n",
    "    tfm_metrics = evaluate_model_comprehensive(tfm_model, test_x, test_y, label_cols, thresholds=opt_thr, name=\"Transformer\")\n",
    "    \n",
    "    # Generate visualizations\n",
    "    plot_metrics_by_emotion(lstm_metrics, title=\"BiLSTM Performance by Emotion\")\n",
    "    plot_metrics_by_emotion(cnn_metrics, title=\"CNN Performance by Emotion\")\n",
    "    plot_metrics_by_emotion(tfm_metrics, title=\"Transformer Performance by Emotion\")\n",
    "    \n",
    "    # Compare models\n",
    "    model_comparison = [(\"BiLSTM\", lstm_metrics), (\"CNN\", cnn_metrics), (\"Transformer\", tfm_metrics)]\n",
    "    plot_model_comparison(model_comparison, title=\"Model Comparison (F1 Score)\")\n",
    "    \n",
    "    # Analyze top performing emotion in detail\n",
    "    top_emotion = tfm_metrics['Emotion'].iloc[0]\n",
    "    top_idx = label_cols.index(top_emotion)\n",
    "    top_threshold = opt_thr[top_idx] if opt_thr is not None else 0.5\n",
    "    \n",
    "    plot_confusion_matrix_for_class(\n",
    "        tfm_model, test_x, test_y, \n",
    "        top_idx, top_emotion, threshold=top_threshold\n",
    "    )\n",
    "    \n",
    "    # ROC curve analysis\n",
    "    plot_roc_curves(tfm_model, test_x, test_y, label_cols, top_n=5)\n",
    "    \n",
    "    # Error analysis\n",
    "    error_analysis = analyze_error_patterns(tfm_model, test_x, test_y, label_cols, thresholds=opt_thr)\n",
    "    \n",
    "    print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────── 14. Save Models for Part 2 ────────────────────\n",
    "def save_model_for_inference(model, word_index, max_len, label_cols, output_dir):\n",
    "    \"\"\"\n",
    "    Save a model and all necessary preprocessing components for later inference.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        word_index: Dictionary mapping words to indices\n",
    "        max_len: Maximum sequence length\n",
    "        label_cols: List of emotion labels\n",
    "        output_dir: Directory to save model and components\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    import pickle\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Save the model\n",
    "    model.save(os.path.join(output_dir, \"model.h5\"))\n",
    "    \n",
    "    # 2. Save vocabulary (word_index)\n",
    "    with open(os.path.join(output_dir, \"word_index.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(word_index, f)\n",
    "    \n",
    "    # 3. Save label columns\n",
    "    with open(os.path.join(output_dir, \"label_cols.json\"), \"w\") as f:\n",
    "        json.dump(label_cols, f)\n",
    "    \n",
    "    # 4. Save configuration parameters\n",
    "    config = {\n",
    "        \"max_len\": max_len,\n",
    "        \"model_type\": model.name if hasattr(model, \"name\") else \"unknown\"\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f)\n",
    "    \n",
    "    # 5. Save optimized thresholds if available\n",
    "    if 'opt_thr' in globals():\n",
    "        np.save(os.path.join(output_dir, \"opt_thresholds.npy\"), opt_thr)\n",
    "    \n",
    "    print(f\"Model and components saved to {output_dir}\")\n",
    "\n",
    "# Save models for Part 2\n",
    "# Uncomment these lines when ready to save models for Part 2\n",
    "# save_model_for_inference(lstm_model, word2idx, MAX_LEN, label_cols, \"../models/lstm_sentiment\")\n",
    "# save_model_for_inference(cnn_model, word2idx, MAX_LEN, label_cols, \"../models/cnn_sentiment\")\n",
    "# save_model_for_inference(tfm_model, word2idx, MAX_LEN, label_cols, \"../models/transformer_sentiment\")\n",
    "\n",
    "class SentimentInferenceModel:\n",
    "    \"\"\"\n",
    "    Class for loading and using saved sentiment models for inference.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_dir):\n",
    "        \"\"\"\n",
    "        Load a model and its components from a directory.\n",
    "        \n",
    "        Args:\n",
    "            model_dir: Directory containing saved model and components\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import json\n",
    "        import pickle\n",
    "        import numpy as np\n",
    "        from tensorflow import keras\n",
    "        import re\n",
    "        \n",
    "        # Load model\n",
    "        self.model = keras.models.load_model(os.path.join(model_dir, \"model.h5\"))\n",
    "        \n",
    "        # Load vocabulary\n",
    "        with open(os.path.join(model_dir, \"word_index.pkl\"), \"rb\") as f:\n",
    "            self.word_index = pickle.load(f)\n",
    "        \n",
    "        # Load label columns\n",
    "        with open(os.path.join(model_dir, \"label_cols.json\"), \"r\") as f:\n",
    "            self.label_cols = json.load(f)\n",
    "        \n",
    "        # Load configuration\n",
    "        with open(os.path.join(model_dir, \"config.json\"), \"r\") as f:\n",
    "            config = json.load(f)\n",
    "            self.max_len = config[\"max_len\"]\n",
    "            self.model_type = config.get(\"model_type\", \"unknown\")\n",
    "        \n",
    "        # Load thresholds if available\n",
    "        thresh_path = os.path.join(model_dir, \"opt_thresholds.npy\")\n",
    "        if os.path.exists(thresh_path):\n",
    "            self.thresholds = np.load(thresh_path)\n",
    "        else:\n",
    "            self.thresholds = None\n",
    "        \n",
    "        print(f\"Loaded {self.model_type} model with {len(self.label_cols)} emotion classes\")\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Apply the same preprocessing as during training.\n",
    "        \n",
    "        Args:\n",
    "            text: Raw text string\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed and padded sequence\n",
    "        \"\"\"\n",
    "        # Apply same cleaning as in training\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'<.*?>', '', text)            # Remove HTML tags\n",
    "        text = re.sub(r'[^a-z0-9\\s]', '', text)      # Remove punctuation/special chars\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()     # Collapse whitespace\n",
    "        \n",
    "        # Convert to sequence\n",
    "        seq = [self.word_index.get(w, 1) for w in text.split()]  # 1 is OOV\n",
    "        \n",
    "        # Pad sequence\n",
    "        padded_seq = (seq[:self.max_len] + [0] * self.max_len)[:self.max_len]\n",
    "        \n",
    "        # Convert to numpy array and reshape for model\n",
    "        return np.array([padded_seq], dtype=np.int16)\n",
    "    \n",
    "    def predict(self, text, threshold=None, top_k=3):\n",
    "        \"\"\"\n",
    "        Predict emotions for a text input.\n",
    "        \n",
    "        Args:\n",
    "            text: Raw text string\n",
    "            threshold: Classification threshold (use class-specific if None)\n",
    "            top_k: Number of top emotions to return\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with predicted emotions and scores\n",
    "        \"\"\"\n",
    "        # Preprocess the text\n",
    "        seq = self.preprocess_text(text)\n",
    "        \n",
    "        # Get model predictions\n",
    "        pred_scores = self.model.predict(seq, verbose=0)[0]\n",
    "        \n",
    "        # Apply thresholds (either per-class or global)\n",
    "        if threshold is None and self.thresholds is not None:\n",
    "            # Use per-class optimized thresholds\n",
    "            emotions_above_threshold = []\n",
    "            for i, score in enumerate(pred_scores):\n",
    "                if score >= self.thresholds[i]:\n",
    "                    emotions_above_threshold.append((self.label_cols[i], float(score)))\n",
    "        else:\n",
    "            # Use fixed threshold (default 0.5 if not provided)\n",
    "            threshold = threshold if threshold is not None else 0.5\n",
    "            emotions_above_threshold = [(self.label_cols[i], float(score)) \n",
    "                                       for i, score in enumerate(pred_scores) \n",
    "                                       if score >= threshold]\n",
    "        \n",
    "        # Get top K emotions\n",
    "        top_emotions = sorted([(self.label_cols[i], float(score)) \n",
    "                              for i, score in enumerate(pred_scores)], \n",
    "                             key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        return {\n",
    "            \"raw_scores\": {self.label_cols[i]: float(score) for i, score in enumerate(pred_scores)},\n",
    "            \"emotions_above_threshold\": emotions_above_threshold,\n",
    "            \"top_emotions\": top_emotions\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
